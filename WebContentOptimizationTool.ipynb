{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ry02024/WebContentOptimizationPyTool/blob/main/WebContentOptimizationTool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#streamlit：類似度の根拠に2つの文章の比較,文章を1フレーズで要約"
      ],
      "metadata": {
        "id": "l9LbUZMA1rxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##環境構築"
      ],
      "metadata": {
        "id": "d_4FA9X01rxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q\n",
        "!pip install pyngrok -q"
      ],
      "metadata": {
        "id": "-0PRTv841rxG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "yXfzNyhE1rxG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')"
      ],
      "metadata": {
        "id": "wTBjGPdv1rxG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# ngrokトークンを設定\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8eacec-c1d0-46d9-8885-aa8bfd3a91b7",
        "id": "SUP7mo2B1rxG"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##実行ファイル"
      ],
      "metadata": {
        "id": "KfkVixYM1rxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###version1"
      ],
      "metadata": {
        "id": "sMerQbWVeyRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "\n",
        "# NLTKリソースのダウンロード\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# コンテンツ収集関数\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_all_text(url):\n",
        "    # ウェブサイトからHTMLを取得\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # HTMLをBeautifulSoupで解析\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # <script>と<style>タグを削除\n",
        "        for script_or_style in soup([\"script\", \"style\"]):\n",
        "            script_or_style.decompose()\n",
        "\n",
        "        # <body>タグからテキストを取得し、余分な空白を削除\n",
        "        body_text = soup.body.get_text(separator=' ', strip=True)\n",
        "        return body_text\n",
        "    else:\n",
        "        # ウェブサイトの内容の取得に失敗した場合\n",
        "        return \"Error: Unable to fetch the content from the website.\"\n",
        "\n",
        "# テキストの要約\n",
        "def text_explanation(text):\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    # 2つの文章の組み合わせを作成\n",
        "    combined_text = f\"以下の文章を1フレーズに要約してください:\\n1. {text}\"\n",
        "\n",
        "    # テキストの類似度とその根拠をリクエスト\n",
        "    response = model.generate_content(combined_text)\n",
        "\n",
        "    # 応答のテキスト部分を返す\n",
        "    return response.text\n",
        "#テキストを分割し、TF-IDFベクトルに変換後、文間の類似度を計算\n",
        "def split_and_vectorize_text(text, min_length=20):\n",
        "    \"\"\"\n",
        "    指定されたテキストを文に分割し、TF-IDFベクトルに変換後、文間の類似度を計算します。\n",
        "\n",
        "    Parameters:\n",
        "    - text: 分割とベクトル化を行う長いテキスト。\n",
        "    - min_length: 分割された各文の最小長。この値以下の長さの文は無視されます。\n",
        "\n",
        "    Returns:\n",
        "    - cosine_similarity_matrix: 各文間のコサイン類似度行列。\n",
        "    \"\"\"\n",
        "\n",
        "    # 文書の分割\n",
        "    sentences = re.split(r\"[。\\.]\", text)\n",
        "    long_sentences = [sentence.strip() for sentence in sentences if len(sentence.strip()) >= min_length]\n",
        "\n",
        "    if len(long_sentences) > 1:\n",
        "        # TF-IDFベクトル化\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(long_sentences)\n",
        "\n",
        "        # 類似度計算\n",
        "        cosine_similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "        return long_sentences, cosine_similarity_matrix\n",
        "    else:\n",
        "        return None\n",
        "#テキスト間を類似度を根拠に比較\n",
        "def get_similarity_explanation_with_score(text1, text2, similarity_score):\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    # 2つの文章の組み合わせを作成\n",
        "    combined_text = f\"\"\"\n",
        "以下の2つのテキストの類似度スコアは{similarity_score}です。このスコアの背後にある理由を、類似点と相違点を含めて詳しく説明してください。また、その説明を以下のテンプレートに従ってください。\n",
        "**類似度スコアの根拠:**\n",
        "[類似度の根拠についての説明]\n",
        "\n",
        "**相違点:**\n",
        "[相違点についての説明]\n",
        "* **範囲:**\n",
        "[相違点の範囲について]\n",
        "* **詳細:**\n",
        "[相違点の詳細について]\n",
        "* **目的:**\n",
        "[相違点の目的について]\n",
        "\n",
        "**類似点:**\n",
        "[類似点についての説明]\n",
        "* **共通主題:**\n",
        "[共通している主題について]\n",
        "\n",
        "**まとめ**\n",
        "[全体のまとめ]\n",
        "\n",
        "1. {text1}\n",
        "2. {text2}\n",
        "\"\"\"\n",
        "    # テキストの類似度とその根拠をリクエスト\n",
        "    response = model.generate_content(combined_text)\n",
        "    # 応答のテキスト部分を返す\n",
        "    return response.text\n",
        "\n",
        "#類似度行列をもとに、各文書に対して最も類似度が高い文書を見つけます。\n",
        "def find_most_similar_documents(similarity_matrix, threshold=0.3):\n",
        "    \"\"\"\n",
        "    与えられた類似度行列をもとに、各文書に対して最も類似度が高い文書を見つけます。\n",
        "\n",
        "    Parameters:\n",
        "    - similarity_matrix: 文書間の類似度行列\n",
        "    - threshold: この類似度スコア以上の文書を考慮する\n",
        "\n",
        "    Returns:\n",
        "    - similar_documents: 各文書に対して最も類似度が高い文書のインデックスと類似度スコアを含むリスト\n",
        "    \"\"\"\n",
        "\n",
        "    similar_documents = []\n",
        "    for i in range(similarity_matrix.shape[0]):\n",
        "        similarity_scores = similarity_matrix[i]\n",
        "        similarity_scores[i] = 0  # 自分自身のスコアを0に設定\n",
        "\n",
        "        # 最も類似度が高い文書のインデックスを取得\n",
        "        most_similar_doc_index = np.argmax(similarity_scores)\n",
        "        most_similar_doc_score = similarity_scores[most_similar_doc_index]\n",
        "\n",
        "        if most_similar_doc_score >= threshold:\n",
        "            similar_documents.append((i, most_similar_doc_index, most_similar_doc_score))\n",
        "\n",
        "    return similar_documents\n",
        "#テキスト表示\n",
        "def display_similar_documents(sentences, similar_documents):\n",
        "    \"\"\"\n",
        "    Display pairs of similar documents using Streamlit.\n",
        "\n",
        "    :param sentences: List of sentences/documents.\n",
        "    :param similar_documents: List of tuples with the format (index1, index2, similarity_score).\n",
        "    \"\"\"\n",
        "    for i, (doc_idx1, doc_idx2, score) in enumerate(similar_documents):\n",
        "        with st.container():\n",
        "            st.write(f\"文章 {doc_idx1 + 1}と文章 {doc_idx2 + 1}の比較\")\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                st.subheader(f\"文章 {doc_idx1 + 1}\")\n",
        "                st.write(f\"{text_explanation(sentences[doc_idx1])}\")\n",
        "            with col2:\n",
        "                st.subheader(f\"文章 {doc_idx2 + 1}\")\n",
        "                st.write(f\"{text_explanation(sentences[doc_idx2])}\")\n",
        "\n",
        "            st.write(f\"類似度スコア: {score:.2f}\")\n",
        "            # Assuming get_similarity_explanation_with_score is a predefined function that takes two texts and a similarity score\n",
        "            explanation = get_similarity_explanation_with_score(sentences[doc_idx1], sentences[doc_idx2], score)\n",
        "            with st.expander(\"類似度の詳細分析\"):\n",
        "                st.write(explanation)\n",
        "\n",
        "# This function is designed to work within a Streamlit app.\n",
        "# It iterates over each pair of similar documents, displays their content side by side, and shows their similarity score.\n",
        "# Additionally, it provides a detailed analysis of their similarity using an expandable Streamlit expander.\n",
        "\n",
        "# Streamlitアプリのメイン部分\n",
        "def main():\n",
        "    # APIキー入力部分\n",
        "    api_key = st.text_input(\"APIキーを入力してください:\", value=\"\", type=\"password\")\n",
        "\n",
        "    genai.configure(api_key=api_key)\n",
        "    st.title('ウェブページテキストスクレイピングと類似度分析アプリ')\n",
        "\n",
        "    # ユーザー入力部分\n",
        "    user_input_url = st.text_input(\"分析するウェブページのURLを入力してください:\")\n",
        "\n",
        "    # 分析ボタン\n",
        "    if st.button('テキスト抽出と類似度分析'):\n",
        "        if user_input_url:\n",
        "            # コンテンツ収集\n",
        "            all_text = scrape_all_text(user_input_url)\n",
        "            if not all_text.startswith(\"Error\"):\n",
        "                #テキストを分割し、TF-IDFベクトルに変換後、文間の類似度を計算\n",
        "                long_sentences, cosine_sim = split_and_vectorize_text(all_text, min_length=20)\n",
        "                # 類似度が高い文書のインデックスと類似度スコアを含むリストを取得\n",
        "                similar_documents = find_most_similar_documents(cosine_sim, threshold=0.3)\n",
        "                # 類似度スコアが0.3以上の場合に結果を表示\n",
        "                display_similar_documents(long_sentences, similar_documents)\n",
        "\n",
        "            else:\n",
        "                st.error(all_text)\n",
        "        else:\n",
        "            st.error(\"URLが入力されていません。\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5f0dc3-9510-4e9a-a096-351659ad6240",
        "id": "r0zTf1FQ1rxH"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###version2"
      ],
      "metadata": {
        "id": "0PLSLDs6evCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "\n",
        "# NLTKリソースのダウンロード\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# コンテンツ収集関数\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_all_text(url):\n",
        "    # ウェブサイトからHTMLを取得\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # HTMLをBeautifulSoupで解析\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # <script>と<style>タグを削除\n",
        "        for script_or_style in soup([\"script\", \"style\"]):\n",
        "            script_or_style.decompose()\n",
        "\n",
        "        # <body>タグからテキストを取得し、余分な空白を削除\n",
        "        body_text = soup.body.get_text(separator=' ', strip=True)\n",
        "        return body_text\n",
        "    else:\n",
        "        # ウェブサイトの内容の取得に失敗した場合\n",
        "        return \"Error: Unable to fetch the content from the website.\"\n",
        "\n",
        "# テキストの要約\n",
        "def text_explanation(text):\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    # 2つの文章の組み合わせを作成\n",
        "    combined_text = f\"以下の文章を1フレーズに要約してください:\\n1. {text}\"\n",
        "\n",
        "    # テキストの類似度とその根拠をリクエスト\n",
        "    response = model.generate_content(combined_text)\n",
        "\n",
        "    # 応答のテキスト部分を返す\n",
        "    return response.text\n",
        "#テキストを分割し、TF-IDFベクトルに変換後、文間の類似度を計算\n",
        "def split_and_vectorize_text(text):\n",
        "    embedding = genai.embed_content(\n",
        "        model=\"models/embedding-001\",\n",
        "        content=text,\n",
        "        task_type=\"SEMANTIC_SIMILARITY\",)\n",
        "    return embedding\n",
        "\n",
        "def calculate_semantic_similarity(embedding1, embedding2):\n",
        "    similarity_score = cosine_similarity([embedding1], [embedding2])[0][0]\n",
        "    return similarity_score\n",
        "\n",
        "#テキスト間を類似度を根拠に比較\n",
        "def get_similarity_explanation_with_score(text1, text2, similarity_score):\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    # 2つの文章の組み合わせを作成\n",
        "    combined_text = f\"\"\"\n",
        "以下の2つのテキストの類似度スコアは{similarity_score}です。このスコアの背後にある理由を、類似点と相違点を含めて詳しく説明してください。また、その説明を以下のテンプレートに従ってください。\n",
        "**類似度スコアの根拠:**\n",
        "[類似度の根拠についての説明]\n",
        "\n",
        "**相違点:**\n",
        "[相違点についての説明]\n",
        "* **範囲:**\n",
        "[相違点の範囲について]\n",
        "* **詳細:**\n",
        "[相違点の詳細について]\n",
        "* **目的:**\n",
        "[相違点の目的について]\n",
        "\n",
        "**類似点:**\n",
        "[類似点についての説明]\n",
        "* **共通主題:**\n",
        "[共通している主題について]\n",
        "\n",
        "**まとめ**\n",
        "[全体のまとめ]\n",
        "\n",
        "1. {text1}\n",
        "2. {text2}\n",
        "\"\"\"\n",
        "    # テキストの類似度とその根拠をリクエスト\n",
        "    response = model.generate_content(combined_text)\n",
        "    # 応答のテキスト部分を返す\n",
        "    return response.text\n",
        "\n",
        "#テキスト表示\n",
        "def display_similar_documents(text1, text2, score):\n",
        "    \"\"\"\n",
        "    Display pairs of similar documents using Streamlit.\n",
        "\n",
        "    :param sentences: List of sentences/documents.\n",
        "    :param similar_documents: List of tuples with the format (index1, index2, similarity_score).\n",
        "    \"\"\"\n",
        "    with st.container():\n",
        "        st.write(f\"文章 {1}と文章 {2}の比較\")\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            st.subheader(f\"文章 {1}\")\n",
        "            st.write(f\"{text_explanation(text1)}\")\n",
        "        with col2:\n",
        "            st.subheader(f\"文章 {2}\")\n",
        "            st.write(f\"{text_explanation(text2)}\")\n",
        "\n",
        "        st.write(f\"類似度スコア: {score:.2f}\")\n",
        "        # Assuming get_similarity_explanation_with_score is a predefined function that takes two texts and a similarity score\n",
        "        explanation = get_similarity_explanation_with_score(text1, text2, score)\n",
        "        with st.expander(\"類似度の詳細分析\"):\n",
        "            st.write(explanation)\n",
        "\n",
        "# This function is designed to work within a Streamlit app.\n",
        "# It iterates over each pair of similar documents, displays their content side by side, and shows their similarity score.\n",
        "# Additionally, it provides a detailed analysis of their similarity using an expandable Streamlit expander.\n",
        "\n",
        "# Streamlitアプリのメイン部分\n",
        "def main():\n",
        "    # APIキー入力部分\n",
        "    api_key = st.text_input(\"APIキーを入力してください:\", value=\"\", type=\"password\")\n",
        "\n",
        "    genai.configure(api_key=api_key)\n",
        "    st.title('ウェブページテキストスクレイピングと類似度分析アプリ')\n",
        "\n",
        "    # ユーザー入力部分\n",
        "    user_input_url1 = st.text_input(\"分析するウェブページのURL1を入力してください:\")\n",
        "    user_input_url2 = st.text_input(\"分析するウェブページのURL2を入力してください:\")\n",
        "\n",
        "    # 分析ボタン\n",
        "    if st.button('テキスト抽出と類似度分析'):\n",
        "        # コンテンツ収集\n",
        "        text1 = scrape_all_text(user_input_url1)\n",
        "        text2 = scrape_all_text(user_input_url2)\n",
        "        #テキストを分割し、TF-IDFベクトルに変換後、文間の類似度を計算\n",
        "        result1 = split_and_vectorize_text(text1[:1000])\n",
        "        result2 = split_and_vectorize_text(text2[:1000])\n",
        "        # result1とresult2から埋め込みベクトルを抽出する\n",
        "        embedding1 = result1['embedding']\n",
        "        embedding2 = result2['embedding']\n",
        "\n",
        "        # それらの埋め込みベクトルを用いて類似度を計算する\n",
        "        similarity = calculate_semantic_similarity(embedding1, embedding2)\n",
        "        display_similar_documents(text1, text2, similarity)\n",
        "\n",
        "    else:\n",
        "        st.error(\"URLが入力されていません。\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZA5H5s-ew4b",
        "outputId": "285cc041-d0b7-47bb-8b0c-1e75210d5547"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##アプリの起動"
      ],
      "metadata": {
        "id": "W4iAda4i1rxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ngrokを介してStreamlitを公開\n",
        "public_url = ngrok.connect(addr='8501')\n",
        "print('Public URL:', public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b301179-a5f5-434d-c4e0-d69fd2b18fe0",
        "id": "17c_nK0C1rxH"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://fef9-34-122-30-28.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlitアプリケーションの起動\n",
        "!streamlit run streamlit_app.py >/dev/null"
      ],
      "metadata": {
        "id": "RL66sp521rxI"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}